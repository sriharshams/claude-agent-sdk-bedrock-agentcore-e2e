{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Online Evaluations with AgentCore\n",
    "\n",
    "In this lab, you'll set up **continuous online evaluation** to monitor the deployed agent's quality in real-time.\n",
    "\n",
    "## What you'll learn\n",
    "- How to configure online evaluations with built-in evaluators\n",
    "- How to generate test interactions for evaluation\n",
    "- How to interpret evaluation metrics in CloudWatch\n",
    "\n",
    "## Architecture\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "    <img src=\"images/architecture_lab5_evaluation.png\" width=\"75%\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport uuid\nimport time\n\nos.environ[\"CLAUDE_CODE_USE_BEDROCK\"] = \"1\"\nos.environ.pop(\"CLAUDECODE\", None)\n\nimport boto3\nfrom boto3.session import Session\n\nboto_session = Session()\nREGION = boto_session.region_name\n\nfrom utils.aws_helpers import get_ssm_parameter, get_or_create_cognito_pool\n\nprint(f\"Region: {REGION}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Evaluation Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from bedrock_agentcore_starter_toolkit import Evaluation, Runtime\n\neval_client = Evaluation(region=REGION)\n\n# Get the runtime ARN from Lab 4 and extract agent ID\nruntime_arn = get_ssm_parameter(\"/app/customersupport/agentcore/runtime_arn\")\nagent_id = runtime_arn.split(\"/\")[-1]\n\nprint(f\"Runtime ARN: {runtime_arn}\")\nprint(f\"Agent ID: {agent_id}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Online Evaluation Configuration\n",
    "\n",
    "We configure three built-in evaluators:\n",
    "- **GoalSuccessRate**: Did the agent achieve the customer's goal?\n",
    "- **Correctness**: Was the information provided accurate?\n",
    "- **ToolSelectionAccuracy**: Did the agent select the right tools?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create online evaluation configuration\ntry:\n    eval_config = eval_client.create_online_config(\n        config_name=\"customer_support_agent_eval\",\n        agent_id=agent_id,\n        evaluator_list=[\n            \"Builtin.GoalSuccessRate\",\n            \"Builtin.Correctness\",\n            \"Builtin.ToolSelectionAccuracy\",\n        ],\n        sampling_rate=1.0,  # 100% for demo; use lower in production\n        config_description=\"Customer support agent online evaluation\",\n        auto_create_execution_role=True,\n    )\n    print(f\"Evaluation config created: {eval_config}\")\nexcept Exception as e:\n    if \"conflict\" in str(e).lower() or \"already exists\" in str(e).lower():\n        print(f\"Evaluation config already exists - continuing with existing config.\")\n    else:\n        print(f\"Error: {e}\")\n\n# List existing configs\ntry:\n    configs = eval_client.list_online_configs()\n    print(f\"\\nOnline evaluation configs: {len(configs)} found\")\n    for c in configs:\n        print(f\"  {c}\")\nexcept Exception as e:\n    print(f\"Could not list configs: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Test Interactions\n",
    "\n",
    "Let's generate several test interactions to populate evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "agentcore_runtime = Runtime()\n\n# Reconfigure to load existing deployment (same params as Lab 4)\nexecution_role_arn = get_ssm_parameter(\"/app/customersupport/agentcore/runtime_execution_role_arn\")\ncognito_config = get_or_create_cognito_pool(refresh_token=True)\n\nagentcore_runtime.configure(\n    entrypoint=\"runtime/app.py\",\n    execution_role=execution_role_arn,\n    auto_create_ecr=True,\n    requirements_file=\"requirements.txt\",\n    region=REGION,\n    agent_name=\"customer_support_agent\",\n    authorizer_configuration={\n        \"customJWTAuthorizer\": {\n            \"allowedClients\": [cognito_config[\"client_id\"]],\n            \"discoveryUrl\": cognito_config[\"discovery_url\"],\n        }\n    },\n    request_header_configuration={\n        \"requestHeaderAllowlist\": [\"Authorization\"]\n    },\n)\n\n# Verify runtime is ready\nstatus = agentcore_runtime.status()\nprint(f\"Runtime status: {status.endpoint.get('status', 'unknown')}\")\n\n# Refresh auth token\naccess_token = cognito_config[\"bearer_token\"]\n\n# Test scenarios\ntest_queries = [\n    \"What are the specifications of your latest laptops?\",\n    \"My headphones aren't connecting via Bluetooth. Can you help?\",\n    \"I want to return a smartphone I bought 2 weeks ago. What's the process?\",\n    \"Can you tell me about your monitor warranty and also search for the best 4K monitors in 2025?\",\n    \"What payment methods do you accept?\",\n]\n\nfor i, query_text in enumerate(test_queries, 1):\n    session_id = str(uuid.uuid4())\n    print(f\"\\n[{i}/{len(test_queries)}] {query_text}\")\n    \n    try:\n        response = agentcore_runtime.invoke(\n            {\"prompt\": query_text, \"actor_id\": \"eval_customer\"},\n            bearer_token=access_token,\n            session_id=session_id,\n        )\n        print(f\"  Response: {str(response)[:100]}...\")\n    except Exception as e:\n        print(f\"  Error: {e}\")\n    \n    time.sleep(2)  # Small delay between requests\n\nprint(\"\\nTest interactions complete!\")\nprint(\"Evaluation results will appear in CloudWatch within a few minutes.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: View Evaluation Results\n",
    "\n",
    "Evaluation metrics are published to CloudWatch. You can view them:\n",
    "\n",
    "1. **CloudWatch Console** > Metrics > `bedrock-agentcore` namespace\n",
    "2. Look for:\n",
    "   - `GoalSuccessRate` - Percentage of successful goal completions\n",
    "   - `Correctness` - Accuracy of provided information\n",
    "   - `ToolSelectionAccuracy` - How well the agent selects appropriate tools\n",
    "\n",
    "### Interpreting Results\n",
    "- **GoalSuccessRate > 80%**: Agent effectively resolves customer queries\n",
    "- **Correctness > 90%**: Information provided is reliable\n",
    "- **ToolSelectionAccuracy > 85%**: Agent consistently picks the right tool\n",
    "\n",
    "![Online Evaluation Dashboard](images/online_evaluations_dashboard.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# List available evaluators\nprint(\"Available Evaluators:\")\nprint(\"-\" * 40)\ntry:\n    evaluators = eval_client.list_evaluators()\n    for ev in evaluators:\n        print(f\"  {ev}\")\nexcept Exception as e:\n    print(f\"  Error listing evaluators: {e}\")\n\n# Check online evaluation configs\nprint(\"\\nOnline Evaluation Configs:\")\nprint(\"-\" * 40)\ntry:\n    configs = eval_client.list_online_configs(agent_id=agent_id)\n    for config in configs:\n        print(f\"  {config}\")\nexcept Exception as e:\n    print(f\"  Error: {e}\")\n\nprint(\"\\nNote: Evaluation results appear in the AgentCore Observability Dashboard.\")\nprint(\"Check CloudWatch > Metrics > bedrock-agentcore namespace for detailed metrics.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you set up online evaluations:\n",
    "\n",
    "1. **Created evaluation config** with 3 built-in evaluators\n",
    "2. **Generated test interactions** across 5 scenarios\n",
    "3. **Learned to interpret** evaluation metrics in CloudWatch\n",
    "\n",
    "In **Lab 6**, we'll build a Streamlit frontend for the agent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
